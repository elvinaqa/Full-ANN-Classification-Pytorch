# -*- coding: utf-8 -*-
"""Income Classification Pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cQOmb7Y1h3jq1TAXVvwx79aT2VijyMIY
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch 
import torch.nn as nn

from google.colab import files

files.upload()

df = pd.read_csv('income.csv')

df

"""I Seperate Categorical and Continous features"""

df.columns

cat_cols = ['sex', 'education', 'marital-status', 'workclass', 'occupation']
cont_cols = ['age', 'education-num', 'hours-per-week']

y_col = ['label']

"""II **Convert categories to category dtypes**"""

for cat in cat_cols:
  df[cat] = df[cat].astype('category')

"""III Shuffle the Dataset"""

from sklearn.utils import shuffle

df = shuffle(df, random_state=101)
df.reset_index(drop=True, inplace=True)
df.head()

cat_szs = [len(df[col].cat.categories) for col in cat_cols]

cat_szs

emb_size = [(size, min(50, (size+1)//2)) for size in cat_szs]

emb_size

"""IV Create a stack of numpy array from categorical values"""

sx = df['sex'].cat.codes.values
ed = df['education'].cat.codes.values
ms = df['marital-status'].cat.codes.values
wc = df['workclass'].cat.codes.values
oc = df['occupation'].cat.codes.values

cats = np.stack([sx, ed, ms, wc, oc], axis=1)

cats

"""**V Convert cats to tensor**"""

cats = torch.tensor(cats, dtype=torch.int64)

cats

"""VI Array for Continous values"""

conts = np.stack([df[col].values for col in cont_cols],axis=1)

"""VII Convert array to tensor"""

conts = torch.tensor(conts, dtype=torch.float)

conts

"""VIII Convert label to tensor"""

y = torch.tensor(df[y_col].values).flatten()

y

"""IX Train and test sets"""

b = 30000
t = 5000

cat_train = cats[:b-t]
cat_test = cats[b-t:b]

cont_train = conts[:b-t]
cont_test = conts[b-t:b]

y_train = y[:b-t]
y_test = y[b-t:b]

"""X Define Model"""

class TabularModel(nn.Module):

  def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):
        # Call the parent __init__
    super().__init__()
        
        # Set up the embedding, dropout, and batch normalization layer attributes
    self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])
    self.emb_drop = nn.Dropout(p)
    self.bn_cont = nn.BatchNorm1d(n_cont)
        
        # Assign a variable to hold a list of layers
    layerlist = []
        
        # Assign a variable to store the number of embedding and continuous layers
    n_emb = sum((nf for ni,nf in emb_szs))
    n_in = n_emb + n_cont
        
        # Iterate through the passed-in "layers" parameter (ie, [200,100]) to build a list of layers
    for i in layers:
        layerlist.append(nn.Linear(n_in,i)) 
        layerlist.append(nn.ReLU(inplace=True))
        layerlist.append(nn.BatchNorm1d(i))
        layerlist.append(nn.Dropout(p))
        n_in = i
    layerlist.append(nn.Linear(layers[-1],out_sz))
        
        # Convert the list of layers into an attribute
    self.layers = nn.Sequential(*layerlist)
    
  def forward(self, x_cat, x_cont):
        # Extract embedding values from the incoming categorical data
    embeddings = []
    for i,e in enumerate(self.embeds):
        embeddings.append(e(x_cat[:,i]))
    x = torch.cat(embeddings, 1)
        # Perform an initial dropout on the embeddings
    x = self.emb_drop(x)
        
        # Normalize the incoming continuous data
    x_cont = self.bn_cont(x_cont)
    x = torch.cat([x, x_cont], 1)
        
        # Set up model layers
    x = self.layers(x)
    return x

torch.manual_seed(33)
model = TabularModel(emb_size, conts.shape[1], 2, [50], 0.4)

"""XI Define loss and optimizer"""

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

"""XII Train the model"""

import time
start_time = time.time()

epochs = 300
losses = []

for i in range(epochs):
    i+=1
    y_pred = model(cat_train, cont_train)
    loss = criterion(y_pred, y_train)
    losses.append(loss)
    
    # a neat trick to save screen space:
    if i%25 == 1:
        print(f'epoch: {i:3}  loss: {loss.item():10.8f}')

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print(f'epoch: {i:3}  loss: {loss.item():10.8f}') # print the last line
print(f'\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed

plt.plot(range(epochs),losses)

"""XIII Evaluate Test set"""

with torch.no_grad():
  y_val = model(cat_test, cont_test)
  loss = criterion(y_val, y_test)
print(f"loss{loss}")

"""XIV Accuracy"""

rows = len(y_test)
correct = 0

for i in range(rows):
  if y_val[i].argmax().item() == y_test[i]:
    correct+=1
print(f"Accuracy: {100*correct/rows}")

"""XV Unseen data"""

# DON'T WRITE HERE
def test_data(mdl): # pass in the name of the model
    # INPUT NEW DATA
    age = float(input("What is the person's age? (18-90)  "))
    sex = input("What is the person's sex? (Male/Female) ").capitalize()
    edn = int(input("What is the person's education level? (3-16) "))
    mar = input("What is the person's marital status? ").capitalize()
    wrk = input("What is the person's workclass? ").capitalize()
    occ = input("What is the person's occupation? ").capitalize()
    hrs = float(input("How many hours/week are worked? (20-90)  "))

    # PREPROCESS THE DATA
    sex_d = {'Female':0, 'Male':1}
    mar_d = {'Divorced':0, 'Married':1, 'Married-spouse-absent':2, 'Never-married':3, 'Separated':4, 'Widowed':5}
    wrk_d = {'Federal-gov':0, 'Local-gov':1, 'Private':2, 'Self-emp':3, 'State-gov':4}
    occ_d = {'Adm-clerical':0, 'Craft-repair':1, 'Exec-managerial':2, 'Farming-fishing':3, 'Handlers-cleaners':4,
            'Machine-op-inspct':5, 'Other-service':6, 'Prof-specialty':7, 'Protective-serv':8, 'Sales':9, 
            'Tech-support':10, 'Transport-moving':11}

    sex = sex_d[sex]
    mar = mar_d[mar]
    wrk = wrk_d[wrk]
    occ = occ_d[occ]

    # CREATE CAT AND CONT TENSORS
    cats = torch.tensor([sex,edn,mar,wrk,occ], dtype=torch.int64).reshape(1,-1)
    conts = torch.tensor([age,hrs], dtype=torch.float).reshape(1,-1)
    
    # SET MODEL TO EVAL (in case this hasn't been done)
    mdl.eval()

    # PASS NEW DATA THROUGH THE MODEL WITHOUT PERFORMING A BACKPROP
    with torch.no_grad():
        z = mdl(cats, conts).argmax().item()

    print(f'\nThe predicted label is {z}')
    
test_data(model)